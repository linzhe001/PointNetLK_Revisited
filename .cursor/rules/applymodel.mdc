---
description: 
globs: 
alwaysApply: false
---

# 自定义特征提取器实现

根据`applymodel.mdc`规则，下面是完整的实现代码：

## 1. 在feature_extraction.py中添加自定义特征提取器

```python
class Custom_Features(FeatureExtractor):
    """
    自定义特征提取网络
    """
    def __init__(self, dim_k=1024):
        super(Custom_Features, self).__init__(dim_k)
        
        # 定义网络层
        # 示例：更深的PointNet架构
        self.mlp1 = MLPNet(3, [64, 64], b_shared=True).layers
        self.mlp2 = MLPNet(64, [128, 128], b_shared=True).layers
        self.mlp3 = MLPNet(128, [dim_k], b_shared=True).layers
        
    def forward(self, points, iter):
        """
        前向传播，提取点云特征
        
        参数:
            points: 输入点云 [B,N,3]
            iter: 迭代标志
            
        返回:
            当iter=-1时: 返回特征和中间结果用于雅可比矩阵计算
            其他情况: 返回特征向量 [B,dim_k]
        """
        x = points.transpose(1, 2) # [B, 3, N]
        
        if iter == -1:
            # 记录每一层的中间结果用于雅可比矩阵计算
            # 1. 第一个MLP块
            x = self.mlp1[0](x)
            A1_x = x
            x = self.mlp1[1](x)
            bn1_x = x
            x = self.mlp1[2](x)
            M1 = (x > 0).type(torch.float)
            
            x = self.mlp1[3](x)
            A1_2_x = x 
            x = self.mlp1[4](x)
            bn1_2_x = x
            x = self.mlp1[5](x)
            M1_2 = (x > 0).type(torch.float)
            
            # 2. 第二个MLP块
            x = self.mlp2[0](x)
            A2_x = x
            x = self.mlp2[1](x)
            bn2_x = x
            x = self.mlp2[2](x)
            M2 = (x > 0).type(torch.float)
            
            x = self.mlp2[3](x)
            A2_2_x = x
            x = self.mlp2[4](x)
            bn2_2_x = x
            x = self.mlp2[5](x)
            M2_2 = (x > 0).type(torch.float)
            
            # 3. 第三个MLP块
            x = self.mlp3[0](x)
            A3_x = x
            x = self.mlp3[1](x)
            bn3_x = x
            x = self.mlp3[2](x)
            M3 = (x > 0).type(torch.float)
            
            # 最大池化
            max_idx = torch.max(x, -1)[-1]
            x = F.max_pool1d(x, x.size(-1))
            x = x.view(x.size(0), -1)

            # 提取权重
            A1 = self.mlp1[0].weight
            A1_2 = self.mlp1[3].weight
            A2 = self.mlp2[0].weight
            A2_2 = self.mlp2[3].weight
            A3 = self.mlp3[0].weight

            # 返回特征和中间结果
            # 注意：需要保持与原始PointNet接口一致的格式，但内容可以不同
            return x, [M1, M1_2, M2, M2_2, M3], \
                   [A1, A1_2, A2, A2_2, A3], \
                   [A1_x, A1_2_x, A2_x, A2_2_x, A3_x], \
                   [bn1_x, bn1_2_x, bn2_x, bn2_2_x, bn3_x], \
                   max_idx
        else:
            # 正常前向传播
            x = self.mlp1(x)
            x = self.mlp2(x)
            x = self.mlp3(x)
            x = F.max_pool1d(x, x.size(-1))
            x = x.view(x.size(0), -1)

            return x
    
    def get_jacobian(self, p0, mask_fn=None, a_fn=None, ax_fn=None, bn_fn=None, max_idx=None, mode="train", 
                     voxel_coords_diff=None, data_type='synthetic', num_points=None):
        """
        计算特征提取的雅可比矩阵
        
        参数与基类相同，但实现方式需要匹配自定义网络结构
        """
        if num_points is None:
            num_points = p0.shape[1]
        batch_size = p0.shape[0]
        dim_k = self.dim_k
        device = p0.device
        
        # 1. 计算变形雅可比矩阵 - 与原始实现相同
        g_ = torch.zeros(batch_size, 6).to(device)
        warp_jac = compute_warp_jac(g_, p0, num_points)   # B x N x 3 x 6
        
        # 2. 计算特征雅可比矩阵 - 需要针对新架构修改
        # 这里需要实现自定义特征提取器的特征雅可比矩阵计算
        # 示例：使用自定义的feature_jac_custom函数
        feature_j = feature_jac_custom(mask_fn, a_fn, ax_fn, bn_fn, device).to(device)
        feature_j = feature_j.permute(0, 3, 1, 2)   # B x N x 6 x K
        
        # 3. 组合得到最终雅可比矩阵 - 与原始实现相同
        J_ = torch.einsum('ijkl,ijkm->ijlm', feature_j, warp_jac)   # B x N x K x 6
        
        # 4. 根据最大池化索引进行处理 - 与原始实现相同
        jac_max = J_.permute(0, 2, 1, 3)   # B x K x N x 6
        jac_max_ = []
        
        for i in range(batch_size):
            jac_max_t = jac_max[i, torch.arange(dim_k), max_idx[i]]
            jac_max_.append(jac_max_t)
        jac_max_ = torch.cat(jac_max_)
        J_ = jac_max_.reshape(batch_size, dim_k, 6)   # B x K x 6
        
        if len(J_.size()) < 3:
            J = J_.unsqueeze(0)
        else:
            J = J_
        
        # 处理真实数据的特殊情况 - 与原始实现相同
        if mode == 'test' and data_type == 'real':
            J_ = J_.permute(1, 0, 2).reshape(dim_k, -1)   # K x (V6)
            warp_condition = cal_conditioned_warp_jacobian(voxel_coords_diff)   # V x 6 x 6
            warp_condition = warp_condition.permute(0,2,1).reshape(-1, 6)   # (V6) x 6
            J = torch.einsum('ij,jk->ik', J_, warp_condition).unsqueeze(0)   # 1 X K X 6
            
        return J


# 实现自定义特征雅可比矩阵计算函数
def feature_jac_custom(M, A, Ax, BN, device):
    """
    自定义特征雅可比矩阵计算
    
    参数:
        M: 激活掩码列表 [M1, M1_2, M2, M2_2, M3]
        A: 权重列表 [A1, A1_2, A2, A2_2, A3]
        Ax: 卷积输出列表 [A1_x, A1_2_x, A2_x, A2_2_x, A3_x]
        BN: 批归一化输出列表 [bn1_x, bn1_2_x, bn2_x, bn2_2_x, bn3_x]
        device: 计算设备
        
    返回:
        特征雅可比矩阵
    """
    # 解包输入列表
    A1, A1_2, A2, A2_2, A3 = A
    M1, M1_2, M2, M2_2, M3 = M
    A1_x, A1_2_x, A2_x, A2_2_x, A3_x = Ax
    bn1_x, bn1_2_x, bn2_x, bn2_2_x, bn3_x = BN

    # 计算每一层的梯度传播
    # 1 x c_in x c_out x 1
    A1 = (A1.T).detach().unsqueeze(-1)
    A1_2 = (A1_2.T).detach().unsqueeze(-1)
    A2 = (A2.T).detach().unsqueeze(-1)
    A2_2 = (A2_2.T).detach().unsqueeze(-1)
    A3 = (A3.T).detach().unsqueeze(-1)

    # 使用自动微分计算批量归一化的梯度
    # B x 1 x c_out x N
    dBN1 = torch.autograd.grad(outputs=bn1_x, inputs=A1_x, grad_outputs=torch.ones(bn1_x.size()).to(device), retain_graph=True)[0].unsqueeze(1).detach()
    dBN1_2 = torch.autograd.grad(outputs=bn1_2_x, inputs=A1_2_x, grad_outputs=torch.ones(bn1_2_x.size()).to(device), retain_graph=True)[0].unsqueeze(1).detach()
    dBN2 = torch.autograd.grad(outputs=bn2_x, inputs=A2_x, grad_outputs=torch.ones(bn2_x.size()).to(device), retain_graph=True)[0].unsqueeze(1).detach()
    dBN2_2 = torch.autograd.grad(outputs=bn2_2_x, inputs=A2_2_x, grad_outputs=torch.ones(bn2_2_x.size()).to(device), retain_graph=True)[0].unsqueeze(1).detach()
    dBN3 = torch.autograd.grad(outputs=bn3_x, inputs=A3_x, grad_outputs=torch.ones(bn3_x.size()).to(device), retain_graph=True)[0].unsqueeze(1).detach()

    # B x 1 x c_out x N
    M1 = M1.detach().unsqueeze(1)
    M1_2 = M1_2.detach().unsqueeze(1)
    M2 = M2.detach().unsqueeze(1)
    M2_2 = M2_2.detach().unsqueeze(1)
    M3 = M3.detach().unsqueeze(1)

    # 使用广播计算 --> B x c_in x c_out x N
    A1BN1M1 = A1 * dBN1 * M1
    A1_2BN1_2M1_2 = A1_2 * dBN1_2 * M1_2
    A2BN2M2 = A2 * dBN2 * M2
    A2_2BN2_2M2_2 = A2_2 * dBN2_2 * M2_2
    A3BN3M3 = A3 * dBN3 * M3

    # 使用einsum组合 - 需要根据自定义网络结构调整
    A1BN1M1_A1_2 = torch.einsum('ijkl,ikml->ijml', A1BN1M1, A1_2BN1_2M1_2)
    A1_2_A2 = torch.einsum('ijkl,ikml->ijml', A1BN1M1_A1_2, A2BN2M2)
    A2_A2_2 = torch.einsum('ijkl,ikml->ijml', A1_2_A2, A2_2BN2_2M2_2)
    A2_2_A3 = torch.einsum('ijkl,ikml->ijml', A2_A2_2, A3BN3M3)
    
    feat_jac = A2_2_A3

    return feat_jac   # B x 3 x K x N
```

## 2. 修改trainer.py中的create_features方法

```python

from feature_extraction import Pointnet_Features, Custom_Features

def create_features(self):
    """
    创建特征提取网络
    
    返回:
        特征提取器实例
    """
    if self.embedding == 'pointnet':
        ptnet = Pointnet_Features(dim_k=self.dim_k)
    elif self.embedding == 'custom':
        ptnet = Custom_Features(dim_k=self.dim_k)
    # 可以在这里添加更多类型的特征提取器
    else:
        raise ValueError(f"未知的特征提取器类型: {self.embedding}")
        
    return ptnet.float()
```

## 3. 修改train.py中的参数帮助文本

```python
# settings for Embedding
parser.add_argument('--embedding', default='pointnet',
                    type=str, help='特征提取器类型: pointnet, custom')
```

## 进一步优化建议

1. **增加批归一化动量参数**:
```python
def __init__(self, dim_k=1024, bn_momentum=0.1):
    # 允许调整批归一化动量参数
    self.mlp1 = MLPNet(3, [64, 64], b_shared=True, bn_momentum=bn_momentum).layers
```

2. **添加Dropout正则化**:
```python
def __init__(self, dim_k=1024, dropout=0.1):
    # 在非共享层后添加Dropout
    self.mlp1 = MLPNet(3, [64, 64], b_shared=True, dropout=dropout).layers
```

3. **实现注意力机制**:
可以在特征提取过程中添加注意力机制，以便更好地关注点云中的关键区域。

4. **多尺度特征融合**:
考虑使用多尺度特征融合，以捕获不同尺度下的点云特征。
